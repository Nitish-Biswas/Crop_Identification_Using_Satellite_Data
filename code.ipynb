{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-17T14:05:54.512593Z",
     "iopub.status.busy": "2025-09-17T14:05:54.512375Z",
     "iopub.status.idle": "2025-09-17T14:05:54.516745Z",
     "shell.execute_reply": "2025-09-17T14:05:54.516130Z",
     "shell.execute_reply.started": "2025-09-17T14:05:54.512571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install terratorch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:16:15.962631Z",
     "iopub.status.busy": "2025-09-17T14:16:15.961950Z",
     "iopub.status.idle": "2025-09-17T14:16:15.965942Z",
     "shell.execute_reply": "2025-09-17T14:16:15.965207Z",
     "shell.execute_reply.started": "2025-09-17T14:16:15.962608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/terratorch3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:16:16.240979Z",
     "iopub.status.busy": "2025-09-17T14:16:16.240453Z",
     "iopub.status.idle": "2025-09-17T14:18:32.904215Z",
     "shell.execute_reply": "2025-09-17T14:18:32.903620Z",
     "shell.execute_reply.started": "2025-09-17T14:16:16.240955Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/terratorch3/pretrainedmodels/models/dpn.py:255: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if block_type is 'proj':\n",
      "/kaggle/input/terratorch3/pretrainedmodels/models/dpn.py:258: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif block_type is 'down':\n",
      "/kaggle/input/terratorch3/pretrainedmodels/models/dpn.py:262: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  assert block_type is 'normal'\n",
      "2025-09-17 14:16:54.510426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758118614.693982      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758118614.759397      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import albumentations\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
    "from terratorch.tasks import SemanticSegmentationTask\n",
    "import zipfile\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:18:32.906193Z",
     "iopub.status.busy": "2025-09-17T14:18:32.905481Z",
     "iopub.status.idle": "2025-09-17T14:18:32.909654Z",
     "shell.execute_reply": "2025-09-17T14:18:32.908994Z",
     "shell.execute_reply.started": "2025-09-17T14:18:32.906171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:18:32.910740Z",
     "iopub.status.busy": "2025-09-17T14:18:32.910440Z",
     "iopub.status.idle": "2025-09-17T14:18:32.927970Z",
     "shell.execute_reply": "2025-09-17T14:18:32.927447Z",
     "shell.execute_reply.started": "2025-09-17T14:18:32.910713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# rm -rf /kaggle/working/crop_17_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:18:32.945810Z",
     "iopub.status.busy": "2025-09-17T14:18:32.945568Z",
     "iopub.status.idle": "2025-09-17T14:22:37.614870Z",
     "shell.execute_reply": "2025-09-17T14:22:37.614221Z",
     "shell.execute_reply.started": "2025-09-17T14:18:32.945781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Starting batch processing for: train\n",
      "Input directory: /kaggle/input/crop-dataset/track1-india-al-impact-gen-ai-hackathon/train/inputs\n",
      "Output directory: /kaggle/working/crop_34_bands/train/inputs\n",
      "Keeping original band indices: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Found 928 TIFF files to process...\n",
      "\n",
      "Processing complete!\n",
      "Successfully processed: 928 files\n",
      "Failed: 0 files\n",
      "--------------------------------------------------\n",
      "Starting batch processing for: val\n",
      "Input directory: /kaggle/input/crop-dataset/track1-india-al-impact-gen-ai-hackathon/val/inputs\n",
      "Output directory: /kaggle/working/crop_34_bands/val/inputs\n",
      "Keeping original band indices: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Found 119 TIFF files to process...\n",
      "\n",
      "Processing complete!\n",
      "Successfully processed: 119 files\n",
      "Failed: 0 files\n",
      "--------------------------------------------------\n",
      "Starting batch processing for: test\n",
      "Input directory: /kaggle/input/crop-dataset/track1-india-al-impact-gen-ai-hackathon/test/inputs\n",
      "Output directory: /kaggle/working/crop_34_bands/test/inputs\n",
      "Keeping original band indices: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Found 126 TIFF files to process...\n",
      "\n",
      "Processing complete!\n",
      "Successfully processed: 126 files\n",
      "Failed: 0 files\n",
      "\n",
      "All script operations completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def calculate_spectral_indices(src):\n",
    "    \"\"\"\n",
    "    Calculates 22 spectral indices from a rasterio source object.\n",
    "        \n",
    "    Assumes standard Sentinel-2 band ordering:\n",
    "    B2=Blue, B3=Green, B4=Red, B5=RE1, B6=RE2, B8=NIR, B11=SWIR1, B12=SWIR2\n",
    "    \"\"\"\n",
    "    # Read required bands into variables with a small epsilon to avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    bands = src.read().astype(np.float32)\n",
    "    \n",
    "    # Sentinel-2 Band Mapping (1-based index to 0-based array index)\n",
    "    blue = bands[1]   # b02\n",
    "    green = bands[2]  # b03\n",
    "    red = bands[3]    # b04\n",
    "    re1 = bands[4]    # b05 (Red-Edge 1)\n",
    "    re2 = bands[5]    # b06 (Red-Edge 2)\n",
    "    b7 = bands[6]\n",
    "    nir = bands[7]    # b08 (NIR)\n",
    "    swir1 = bands[10] # b11 (SWIR 1)\n",
    "    swir2 = bands[11] # b12 (SWIR 2)\n",
    "\n",
    "    # --- Calculate Indices ---\n",
    "    # Suppress runtime warnings for invalid divisions, which we handle with np.nan_to_num\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        # --- Original 11 Indices ---\n",
    "        ndvi = (nir - red) / (nir + red + epsilon)\n",
    "        evi = 2.5 * (nir - red) / (nir + 6 * red - 7.5 * blue + 1 + epsilon)\n",
    "        L = 0.5\n",
    "        savi = ((nir - red) / (nir + red + L + epsilon)) * (1 + L)\n",
    "        ndmi = (nir - swir1) / (nir + swir1 + epsilon)\n",
    "        gvmi = ((nir + 0.1) - (swir2 + 0.02)) / ((nir + 0.1) + (swir2 + 0.02) + epsilon)\n",
    "        ndre1 = (nir - re1) / (nir + re1 + epsilon)\n",
    "        ndre2 = (nir - re2) / (nir + re2 + epsilon)\n",
    "        ndyi = (green - blue) / (green + blue + epsilon)\n",
    "        psri = (red - blue) / (re2 + epsilon)\n",
    "        gndvi = (nir - green) / (nir + green + epsilon)\n",
    "        rep = 705 + 35 * (((red + b7) / 2) - re1) / (re2 - re1 + epsilon)\n",
    "\n",
    "        # --- START: Added 11 New Indices ---\n",
    "        osavi = (nir - red) / (nir + red + 0.16 + epsilon)\n",
    "        wdrvi = (0.1 * nir - red) / (0.1 * nir + red + epsilon)\n",
    "        mcari = (re1 - red - 0.2 * (re1 - green)) * (re1 / (red + epsilon))\n",
    "        tcari = 3 * ((re1 - red) - 0.2 * (re1 - green) * (re1 / (red + epsilon)))\n",
    "        \n",
    "        # CCCI is a ratio of NDRE1 and NDVI\n",
    "        ccci_numerator = (nir - re1) / (nir + re1 + epsilon) # NDRE1\n",
    "        ccci_denominator = (nir - red) / (nir + red + epsilon) # NDVI\n",
    "        ccci = ccci_numerator / (ccci_denominator + epsilon)\n",
    "\n",
    "        mtci = (re2 - re1) / (re1 - red + epsilon)\n",
    "        cire = (nir / (re1 + epsilon)) - 1\n",
    "        sipi = (nir - blue) / (nir - red + epsilon)\n",
    "        bsi = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue) + epsilon)\n",
    "        exg = 2 * green - red - blue\n",
    "        rdvi = (nir - red) / np.sqrt(nir + red + epsilon)\n",
    "        # Note: NDVIre is the same formula as NDRE1, so it is not added again.\n",
    "        # --- END: Added 11 New Indices ---\n",
    "\n",
    "    indices = [\n",
    "        # Original\n",
    "        ndvi, evi, savi, ndmi, gvmi, ndre1, ndre2,\n",
    "        ndyi, psri, gndvi, rep,\n",
    "        # New\n",
    "        osavi, wdrvi, mcari, tcari, ccci, mtci,\n",
    "        cire, sipi, bsi, exg, rdvi\n",
    "    ]\n",
    "    \n",
    "    # Handle potential NaN/Inf values that may have resulted from division issues\n",
    "    indices_cleaned = [np.nan_to_num(idx) for idx in indices]\n",
    "    \n",
    "    return np.array(indices_cleaned, dtype=np.float32)\n",
    "\n",
    "def select_add_indices_and_save(input_path, output_path, band_indices_to_keep):\n",
    "    \"\"\"\n",
    "    Selects 6 bands, calculates 22 indices from the original 12 bands,\n",
    "    and saves a new 28-band TIFF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(input_path) as src:\n",
    "            if src.count < 12:\n",
    "                print(f\"Error: Input file {input_path} has {src.count} bands. At least 12 are required.\")\n",
    "                return False\n",
    "\n",
    "            # Read the 6 bands you want to keep\n",
    "            selected_bands = src.read(band_indices_to_keep)\n",
    "            \n",
    "            # Calculate the 22 new index bands from the full source\n",
    "            index_bands = calculate_spectral_indices(src)\n",
    "\n",
    "            # Stack the 6 selected bands and the 22 new index bands\n",
    "            all_28_bands = np.vstack((selected_bands.astype(np.float32), index_bands))\n",
    "\n",
    "            # Update profile for the new file\n",
    "            profile = src.profile.copy()\n",
    "            profile.update(\n",
    "                count=34, # UPDATED BAND COUNT\n",
    "                dtype=np.float32 # Use float32 to accommodate index values\n",
    "            )\n",
    "\n",
    "            # Create the output directory if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "            # Write the new 28-band TIFF file\n",
    "            with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                dst.write(all_28_bands)\n",
    "        \n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {input_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_directory(input_dir, output_dir, band_indices_to_keep, pattern=\"*.tif\"):\n",
    "    \"\"\"\n",
    "    Processes all TIFFs in a directory to create 28-band files.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "\n",
    "    if not input_path.exists():\n",
    "        print(f\"Error: Input directory '{input_dir}' does not exist!\")\n",
    "        return\n",
    "\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    tiff_files = list(input_path.glob(pattern))\n",
    "    \n",
    "    if not tiff_files:\n",
    "        print(f\"No TIFF files found matching pattern '{pattern}' in {input_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(tiff_files)} TIFF files to process...\")\n",
    "    successful, failed = 0, 0\n",
    "\n",
    "    for tiff_file in tiff_files:\n",
    "        output_file = output_path / tiff_file.name\n",
    "        if select_add_indices_and_save(str(tiff_file), str(output_file), band_indices_to_keep):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Successfully processed: {successful} files\")\n",
    "    print(f\"Failed: {failed} files\")\n",
    "\n",
    "# --- Example usage and configuration ---\n",
    "if __name__ == \"__main__\":\n",
    "    # The bands selected to be kept in the final file.\n",
    "    # The calculation step still uses all original bands as needed.\n",
    "    BANDS_TO_SELECT = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] # Blue, Green, Red, NIR, SWIR1, SWIR2\n",
    "\n",
    "    base_dir = '/kaggle/input/crop-dataset/track1-india-al-impact-gen-ai-hackathon'\n",
    "    out_dir = '/kaggle/working/crop_34_bands' # UPDATED output directory name\n",
    "    sub_dirs = ['train', 'val', 'test']\n",
    "    \n",
    "    for sub in sub_dirs:\n",
    "        input_directory = os.path.join(base_dir, sub, 'inputs')\n",
    "        output_directory = os.path.join(out_dir, sub, 'inputs')\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Starting batch processing for: {sub}\")\n",
    "        print(f\"Input directory: {input_directory}\")\n",
    "        print(f\"Output directory: {output_directory}\")\n",
    "        print(f\"Keeping original band indices: {BANDS_TO_SELECT}\")\n",
    "        \n",
    "        process_directory(input_directory, output_directory, BANDS_TO_SELECT)\n",
    "\n",
    "    print(\"\\nAll script operations completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:22:37.615888Z",
     "iopub.status.busy": "2025-09-17T14:22:37.615646Z",
     "iopub.status.idle": "2025-09-17T14:22:46.828199Z",
     "shell.execute_reply": "2025-09-17T14:22:46.827404Z",
     "shell.execute_reply.started": "2025-09-17T14:22:37.615862Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '/kaggle/input/crop-dataset/track1-india-al-impact-gen-ai-hackathon/train/labels' copied successfully to '/kaggle/working/crop_34_bands/train/labels'.\n",
      "Folder '/kaggle/input/crop-dataset/track1-india-al-impact-gen-ai-hackathon/val/labels' copied successfully to '/kaggle/working/crop_34_bands/val/labels'.\n",
      "Folder '/kaggle/input/crop-dataset/track1-india-al-impact-gen-ai-hackathon/test/labels' copied successfully to '/kaggle/working/crop_34_bands/test/labels'.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def copy_folder(source_folder, destination_folder):\n",
    "    \"\"\"\n",
    "    Copies an entire folder from a source path to a destination path.\n",
    "\n",
    "    Args:\n",
    "        source_folder (str): The path to the source folder.\n",
    "        destination_folder (str): The path to the destination folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Check if the destination folder already exists\n",
    "        if os.path.exists(destination_folder):\n",
    "            print(f\"Warning: Destination folder '{destination_folder}' already exists. \"\n",
    "                  \"If you want to overwrite, you might need to remove it first.\")\n",
    "            # Optionally, you could add logic here to remove the existing destination\n",
    "            # shutil.rmtree(destination_folder)\n",
    "            # print(f\"Removed existing destination folder: {destination_folder}\")\n",
    "        \n",
    "        # Copy the entire directory tree\n",
    "        shutil.copytree(source_folder, destination_folder)\n",
    "        print(f\"Folder '{source_folder}' copied successfully to '{destination_folder}'.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Error: Destination folder '{destination_folder}' already exists. \"\n",
    "              \"Use `dirs_exist_ok=True` in shutil.copytree to allow overwriting (Python 3.8+), \"\n",
    "              \"or remove the destination folder first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "base_dir = '/kaggle/input/crop-dataset/track1-india-al-impact-gen-ai-hackathon'\n",
    "out_dir = '/kaggle/working/crop_34_bands' # Changed output dir name for clarity\n",
    "sub_dirs = ['train', 'val', 'test']\n",
    "\n",
    "for sub in sub_dirs:\n",
    "    input_directory = os.path.join(base_dir, sub, 'labels')\n",
    "    output_directory = os.path.join(out_dir, sub, 'labels')\n",
    "\n",
    "    copy_folder(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:22:46.829258Z",
     "iopub.status.busy": "2025-09-17T14:22:46.828985Z",
     "iopub.status.idle": "2025-09-17T14:22:46.857235Z",
     "shell.execute_reply": "2025-09-17T14:22:46.856742Z",
     "shell.execute_reply.started": "2025-09-17T14:22:46.829241Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "All script operations completed!\n",
      "--------------------------------------------------\n",
      "\n",
      "All script operations completed!\n",
      "--------------------------------------------------\n",
      "\n",
      "All script operations completed!\n"
     ]
    }
   ],
   "source": [
    "base_dir = '/kaggle/input/crop-dataset/track1-india-al-impact-gen-ai-hackathon'\n",
    "out_dir = '/kaggle/working/crop_34_bands' # Changed output dir name for clarity\n",
    "sub_dirs = ['train.txt', 'val.txt', 'test.txt']\n",
    "\n",
    "for sub in sub_dirs:\n",
    "    input_directory = os.path.join(base_dir, sub)\n",
    "    output_directory = os.path.join(out_dir, sub)\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    shutil.copyfile(input_directory, output_directory)\n",
    "    print(\"\\nAll script operations completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:22:46.858086Z",
     "iopub.status.busy": "2025-09-17T14:22:46.857875Z",
     "iopub.status.idle": "2025-09-17T14:22:46.927604Z",
     "shell.execute_reply": "2025-09-17T14:22:46.926875Z",
     "shell.execute_reply.started": "2025-09-17T14:22:46.858072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path = '/kaggle/working/crop_34_bands/'\n",
    "\n",
    "datamodule = GenericNonGeoSegmentationDataModule(\n",
    "    batch_size=10,\n",
    "    num_workers=2,\n",
    "    num_classes=6,\n",
    "\n",
    "    # Define dataset paths\n",
    "    train_data_root=dataset_path+'/train/inputs',\n",
    "    train_label_data_root=dataset_path+'/train/labels',\n",
    "    val_data_root=dataset_path+'/val/inputs',\n",
    "    val_label_data_root=dataset_path+'/val/labels',\n",
    "    test_data_root=dataset_path+'/val/inputs',\n",
    "    test_label_data_root=dataset_path+'/val/labels',\n",
    "    \n",
    "    #data set path for infereencing on test input\n",
    "    predict_data_root=dataset_path+'test/inputs',\n",
    " \n",
    "    # Define splits\n",
    "    train_split=dataset_path+\"/train.txt\",\n",
    "    val_split=dataset_path+\"/val.txt\",\n",
    "    test_split=dataset_path+\"/val.txt\",\n",
    "\n",
    "    \n",
    "    img_grep='*input.tif',\n",
    "    label_grep='*label_c6.tif',\n",
    "\n",
    "    train_transform=[\n",
    "        albumentations.D4(),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    val_transform=None,\n",
    "    test_transform=None,\n",
    "    means = [43.414087986123974, 38.81009859874331, 37.657296657562256, 39.46452532143429, 42.66194085417123, 54.86814571249074, 63.38708205058657, 60.08845993568157, 13.279416692667994, 69.287048570041, 48.36614383500198, 69.84629751073903, 0.2132587335839987, 26370903.853171803, 0.3182134306061422, 0.13085361243484275, -0.07412845801290406, 0.17232999177350547, 0.04472540249474705, -0.017443628418975062, 0.004647128527066735, 0.22877457022055528, 5956769.690392101, 0.21290016207165716, -0.7262815527178053, 2.638937688962349, 6.315807526144584, -312.4690136898994, 61684051.37808944, 0.4463677766159984, -372476.07332120626, -0.07555472765309695, -2.9600306050530794, 2.0925896116181475],\n",
    "    stds = [3.249200333906643, 4.092319172521173, 5.404914137661227, 9.249756503354499, 8.044317368830914, 6.790147526536125, 8.10124124355119, 7.866002415825067, 2.4995222018345795, 17.069685938871878, 15.658058630599031, 9.297859936811728, 0.1108298246651591, 410681869.75464195, 0.16528027862984687, 0.1735479324223879, 0.018316452678591062, 0.08676941487768036, 0.027369521891671803, 0.030296780356157682, 0.12347729336499248, 0.07469651735070214, 223392898.5723597, 0.1106232296227927, 0.05979664023763734, 3.251688884134678, 7.749837612234008, 66721.16934437146, 229531054.1366095, 0.28962555049655164, 20673357.31535897, 0.1297231175522108, 3.3168257425479246, 1.033343191206589],\n",
    "    no_data_replace=0,\n",
    "    no_label_replace=-1,\n",
    ")\n",
    "\n",
    "\n",
    "datamodule.setup(\"fit\")\n",
    "datamodule.setup(\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:22:46.928513Z",
     "iopub.status.busy": "2025-09-17T14:22:46.928317Z",
     "iopub.status.idle": "2025-09-17T14:22:46.935374Z",
     "shell.execute_reply": "2025-09-17T14:22:46.934824Z",
     "shell.execute_reply.started": "2025-09-17T14:22:46.928490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 928\n",
      "Validation set size: 119\n",
      "Test set size: 119\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Train set size:\", len(datamodule.train_dataset))\n",
    "print(\"Validation set size:\", len(datamodule.val_dataset))\n",
    "\n",
    "datamodule.setup(\"test\")\n",
    "print(\"Test set size:\", len(datamodule.test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:22:46.937959Z",
     "iopub.status.busy": "2025-09-17T14:22:46.937731Z",
     "iopub.status.idle": "2025-09-17T14:22:46.956500Z",
     "shell.execute_reply": "2025-09-17T14:22:46.955926Z",
     "shell.execute_reply.started": "2025-09-17T14:22:46.937943Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "OUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "EPOCHS = 120\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "HEAD_DROPOUT = 0.25\n",
    "FREEZE_BACKBONE = False\n",
    "\n",
    "BANDS =[\n",
    "        \"B01\",\n",
    "        \"BLUE\",\n",
    "        \"GREEN\",\n",
    "        \"RED\",\n",
    "        \"B05\",\n",
    "        \"B06\",\n",
    "        \"B07\",\n",
    "        \"NIR_NARROW\",\n",
    "        \"B8A\",\n",
    "        \"B09\",\n",
    "        \"SWIR_1\",\n",
    "        \"SWIR_2\",\n",
    "        'NDVI',   \n",
    "        'EVI',    \n",
    "        'SAVI',   \n",
    "        'NDMI',   \n",
    "        'GVMI',   \n",
    "        'NDRE1', \n",
    "        'NDRE2', \n",
    "        'NDYI',  \n",
    "        'PSRI',  \n",
    "        'GNDVI', \n",
    "        'REP',\n",
    "        'OSAVI',\n",
    "        'WDRVI',\n",
    "        'MCARI',\n",
    "        'TCARI',\n",
    "        'CCCI',\n",
    "        'MTCI',\n",
    "        'CIre', \n",
    "        'SIPI', \n",
    "        'BSI',\n",
    "        'EXG',\n",
    "        'RDVI', \n",
    "    ]\n",
    "NUM_FRAMES = 1\n",
    "\n",
    "#      Crop     Pixel_total    Pixel_percentage Class_weight\n",
    "#      Gram        2545             1.73       0.4761\n",
    "#     Maize        7128             4.84       0.1702\n",
    "#   Mustard       36362            24.67       0.0334\n",
    "# Sugarcane        4542             3.08       0.2674\n",
    "#     Wheat       59585            40.42       0.0204\n",
    "# OtherCrop       37247            25.27       0.0326\n",
    "\n",
    "CLASS_WEIGHTS = [0.40, 0.15, 0.05, 0.35, 0.03, 0.04]\n",
    "\n",
    "SEED = 0\n",
    "pl.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:22:46.957815Z",
     "iopub.status.busy": "2025-09-17T14:22:46.957339Z",
     "iopub.status.idle": "2025-09-17T14:22:46.961274Z",
     "shell.execute_reply": "2025-09-17T14:22:46.960521Z",
     "shell.execute_reply.started": "2025-09-17T14:22:46.957782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "# lr_monitor = LearningRateMonitor(logging_interval='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:22:46.962388Z",
     "iopub.status.busy": "2025-09-17T14:22:46.962126Z",
     "iopub.status.idle": "2025-09-17T14:22:46.974298Z",
     "shell.execute_reply": "2025-09-17T14:22:46.973648Z",
     "shell.execute_reply.started": "2025-09-17T14:22:46.962367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SEED = 0\n",
    "\n",
    "# pl.seed_everything(SEED)\n",
    "\n",
    "# # Logger\n",
    "# logger = TensorBoardLogger(\n",
    "#     save_dir=OUT_DIR,\n",
    "#     name=\"crop\",\n",
    "# )\n",
    "\n",
    "# # Callbacks\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     monitor=\"val/Multiclass_Jaccard_Index\",\n",
    "#     mode=\"max\",\n",
    "#     dirpath=os.path.join(OUT_DIR, \"cropid\", \"checkpoints\"),\n",
    "#     filename=\"best-checkpoint-{epoch:02d}-{val_loss:.2f}\",\n",
    "#     save_top_k=1,\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:22:46.975177Z",
     "iopub.status.busy": "2025-09-17T14:22:46.974944Z",
     "iopub.status.idle": "2025-09-17T14:22:47.003916Z",
     "shell.execute_reply": "2025-09-17T14:22:47.003276Z",
     "shell.execute_reply.started": "2025-09-17T14:22:46.975156Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=OUT_DIR,\n",
    "    name=\"crop\",\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val/Multiclass_Jaccard_Index\",\n",
    "    mode=\"max\",\n",
    "    dirpath=os.path.join(OUT_DIR, \"cropid\", \"checkpoints\"),\n",
    "    filename=\"best-checkpoint-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:22:47.004779Z",
     "iopub.status.busy": "2025-09-17T14:22:47.004563Z",
     "iopub.status.idle": "2025-09-17T14:22:47.007713Z",
     "shell.execute_reply": "2025-09-17T14:22:47.007044Z",
     "shell.execute_reply.started": "2025-09-17T14:22:47.004758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# rm -rf /kaggle/working/crop_17_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:25:46.094746Z",
     "iopub.status.busy": "2025-09-17T14:25:46.094156Z",
     "iopub.status.idle": "2025-09-17T14:25:46.101207Z",
     "shell.execute_reply": "2025-09-17T14:25:46.100527Z",
     "shell.execute_reply.started": "2025-09-17T14:25:46.094721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Correct imports\n",
    "from terratorch.registry import TERRATORCH_NECK_REGISTRY\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:25:48.698767Z",
     "iopub.status.busy": "2025-09-17T14:25:48.698487Z",
     "iopub.status.idle": "2025-09-17T14:25:48.706389Z",
     "shell.execute_reply": "2025-09-17T14:25:48.705755Z",
     "shell.execute_reply.started": "2025-09-17T14:25:48.698746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalTverskyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Tversky Loss for imbalanced multi-class semantic segmentation.\n",
    "    Applies focal mechanism directly to Tversky Index, with class weighting.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, class_weights, alpha=0.3, beta=0.7, gamma=4/3, epsilon=1e-7, ignore_index=-1):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha  # Weight on FP (lower to improve recall for rare classes)\n",
    "        self.beta = beta    # Weight on FN (higher to penalize missing rare class pixels)\n",
    "        self.gamma = gamma  # Focal parameter (>1 to focus on hard examples)\n",
    "        self.epsilon = epsilon\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "        if not isinstance(class_weights, torch.Tensor):\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "        self.register_buffer('class_weights', class_weights)  # Register as buffer to move with module.to(device)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs shape: [B, C, H, W], targets shape: [B, H, W]\n",
    "        \n",
    "        # Reshape and filter ignored pixels\n",
    "        logits_flat = inputs.permute(0, 2, 3, 1).contiguous().view(-1, self.num_classes)\n",
    "        targets_flat = targets.view(-1)\n",
    "        \n",
    "        valid_mask = targets_flat != self.ignore_index\n",
    "        logits_flat = logits_flat[valid_mask]\n",
    "        targets_flat = targets_flat[valid_mask]\n",
    "\n",
    "        if targets_flat.numel() == 0:\n",
    "            return inputs.sum() * 0.0\n",
    "\n",
    "        probs = F.softmax(logits_flat, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets_flat, num_classes=self.num_classes).float()\n",
    "        \n",
    "        # Compute per-class sums (across all pixels)\n",
    "        tp = torch.sum(probs * targets_one_hot, dim=0)\n",
    "        fp = torch.sum(probs * (1 - targets_one_hot), dim=0)\n",
    "        fn = torch.sum((1 - probs) * targets_one_hot, dim=0)\n",
    "        \n",
    "        # Per-class Tversky Index\n",
    "        tversky_index = (tp + self.epsilon) / (tp + self.alpha * fp + self.beta * fn + self.epsilon)\n",
    "        \n",
    "        # Focal Tversky per class: (1 - TI)^{1/gamma}\n",
    "        ftl_per_class = (1 - tversky_index) ** (1 / self.gamma)\n",
    "        \n",
    "        # Weighted average across classes\n",
    "        weighted_ftl = torch.sum(self.class_weights * ftl_per_class) / torch.sum(self.class_weights)\n",
    "        \n",
    "        return weighted_ftl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:26:35.020218Z",
     "iopub.status.busy": "2025-09-17T14:26:35.019313Z",
     "iopub.status.idle": "2025-09-17T14:26:35.028418Z",
     "shell.execute_reply": "2025-09-17T14:26:35.027533Z",
     "shell.execute_reply.started": "2025-09-17T14:26:35.020181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from terratorch.tasks import SemanticSegmentationTask\n",
    "\n",
    "class CustomSegmentationTaskWithEQLv2(SemanticSegmentationTask):\n",
    "    def configure_losses(self) -> None:\n",
    "        \"\"\"\n",
    "        This method overrides the original loss configuration from the parent class.\n",
    "        It now also passes the correct 'ignore_index' to the EQLv2 loss.\n",
    "        \"\"\"\n",
    "        # cls_num_list = [2545, 7128, 36362, 4542, 59585, 37247]\n",
    "        # num_classes = self.hparams.model_args[\"num_classes\"]\n",
    "        # # ignore_index = self.hparams.get(\"ignore_index\", -1)\n",
    "        # alpha = torch.tensor([\n",
    "        #     50.0,   # Class 0 - rarest crop (2545 pixels) \n",
    "        #     25.0,   # Class 1 - (7128 pixels)\n",
    "        #     10.0,    # Class 2 - (36362 pixels)\n",
    "        #     40.0,   # Class 3 - (4542 pixels) \n",
    "        #     8.0,    # Class 4 - most common crop (59585 pixels)\n",
    "        #     10.0    # Class 5 - (37247 pixels)\n",
    "        # ], dtype=torch.float32)\n",
    "        \n",
    "        # self.criterion = FocalDiceLoss(\n",
    "        #                 alpha=alpha,\n",
    "        #                 gamma=2.5,          # Higher gamma for extreme imbalance\n",
    "        #                 focal_weight=0.2,   # Emphasize focal loss for imbalance\n",
    "        #                 dice_weight=0.8,    # Lower dice weight\n",
    "        #             )\n",
    "        CLASS_WEIGHTS = [0.40, 0.15, 0.05, 0.35, 0.03, 0.04]\n",
    "        NUM_CLASSES = 6\n",
    "        \n",
    "        self.criterion = FocalTverskyLoss(\n",
    "            num_classes=NUM_CLASSES, \n",
    "            class_weights=CLASS_WEIGHTS,\n",
    "            alpha=0.3,\n",
    "            beta=0.7,\n",
    "            gamma=4/3\n",
    "        )\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Get current LR from the optimizer\n",
    "        current_lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        # Log it (will show in console and TensorBoard)\n",
    "        self.log('lr', current_lr, prog_bar=True, logger=True)\n",
    "        print(f\"INFO: Epoch {self.current_epoch}, Current LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Call super to keep original behavior\n",
    "        super().on_train_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:26:36.969299Z",
     "iopub.status.busy": "2025-09-17T14:26:36.969038Z",
     "iopub.status.idle": "2025-09-17T14:26:38.903387Z",
     "shell.execute_reply": "2025-09-17T14:26:38.902606Z",
     "shell.execute_reply.started": "2025-09-17T14:26:36.969280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Custom neck registered with TerraTorch!\n"
     ]
    }
   ],
   "source": [
    "# Correct imports\n",
    "from terratorch.registry import TERRATORCH_NECK_REGISTRY\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the neck class\n",
    "class CropVIAttentionNeck(nn.Module):\n",
    "    \"\"\"Crop-specific VI attention neck for Prithvi model\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, num_classes=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Handle in_channels as list\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = [in_channels]\n",
    "        self.in_channels_list = in_channels\n",
    "        \n",
    "        # Your exact VI importance weights\n",
    "        self.crop_vi_importance = {\n",
    "            0: [0.9, 0.8, 0.8, 0.8, 0.8, 0.7, 0.7, 0.0, 0.6, 0.8, 0.7, 0.9, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.7, 0.3, 0.5, 0.8],  # Gram\n",
    "            1: [0.9, 1.0, 0.7, 0.7, 0.7, 0.8, 0.8, 0.0, 0.6, 0.8, 0.7, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.9, 0.7, 0.3, 0.5, 0.9],  # Maize  \n",
    "            2: [0.8, 0.8, 0.6, 0.7, 0.7, 0.6, 0.6, 1.0, 0.7, 0.9, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.8, 0.8, 0.7, 0.3, 0.6, 0.7],  # Mustard\n",
    "            3: [1.0, 0.9, 0.7, 0.8, 0.8, 0.7, 0.7, 0.0, 0.6, 0.8, 0.7, 0.8, 0.8, 0.7, 0.7, 0.7, 0.7, 0.7, 0.6, 0.3, 0.5, 0.8],  # Sugarcane\n",
    "            4: [0.9, 0.8, 0.7, 0.7, 0.7, 0.8, 0.7, 0.0, 0.6, 1.0, 0.8, 0.8, 0.9, 0.7, 0.7, 0.7, 0.8, 0.8, 0.7, 0.3, 0.5, 0.8],  # Wheat\n",
    "            5: [0.8, 0.7, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.7, 0.6, 0.5, 0.5, 0.4, 0.4, 0.4, 0.5, 0.4, 0.4, 0.3, 0.4, 0.5]   # Other\n",
    "        }\n",
    "        \n",
    "        # Convert to tensor [6, 11] for VI bands\n",
    "        vi_weights = torch.tensor([self.crop_vi_importance[i] for i in range(6)], dtype=torch.float32)\n",
    "        self.register_buffer('vi_importance_weights', vi_weights)\n",
    "        \n",
    "        # Attention layers per level, avoiding inplace\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        for ch in self.in_channels_list:\n",
    "            reduced_ch = max(ch // 4, 1)\n",
    "            self.attention_layers.append(nn.Sequential(\n",
    "                nn.Linear(ch, reduced_ch),\n",
    "                nn.ReLU(inplace=False),  # Explicitly out-of-place\n",
    "                nn.Linear(reduced_ch, ch),\n",
    "                nn.Sigmoid()\n",
    "            ))\n",
    "        \n",
    "    def process_channel_list(self, in_channels: list[int]) -> list[int]:\n",
    "        \"\"\"Required by TerraTorch - no channel changes\"\"\"\n",
    "        return in_channels\n",
    "        \n",
    "    def forward(self, inputs: list[torch.Tensor]) -> list[torch.Tensor]:\n",
    "        \"\"\"Forward pass - handles list of token features [B, N, D]\"\"\"\n",
    "        outputs = []\n",
    "        for level_idx, x in enumerate(inputs):\n",
    "            # x is [B, N, D] for tokens\n",
    "            B, N, D = x.shape\n",
    "            \n",
    "            # Global average pooling over tokens (out-of-place)\n",
    "            global_features = torch.mean(x, dim=1)  # [B, D]\n",
    "            \n",
    "            # Channel attention (on D dimension, out-of-place)\n",
    "            channel_weights = self.attention_layers[level_idx](global_features)  # [B, D]\n",
    "            \n",
    "            # Apply VI importance (out-of-place with clone)\n",
    "            avg_vi_importance = torch.mean(self.vi_importance_weights, dim=0)  # [11]\n",
    "            if D >= 11:\n",
    "                vi_weights = channel_weights[:, -22:].clone() * avg_vi_importance\n",
    "                channel_weights = channel_weights.clone()\n",
    "                channel_weights[:, -22:] = vi_weights\n",
    "            \n",
    "            # Reshape and apply attention (out-of-place)\n",
    "            attention = channel_weights.unsqueeze(1)  # [B, 1, D]\n",
    "            attended_features = x * attention\n",
    "            \n",
    "            outputs.append(attended_features)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Define a constructor function\n",
    "def create_crop_vi_attention_neck(in_channels=[1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\n",
    ", num_classes=6, **kwargs):\n",
    "    \"\"\"Constructor for CropVIAttentionNeck\"\"\"\n",
    "    return CropVIAttentionNeck(in_channels=in_channels, num_classes=num_classes, **kwargs)\n",
    "\n",
    "# Register it\n",
    "TERRATORCH_NECK_REGISTRY.register( create_crop_vi_attention_neck)\n",
    "\n",
    "print(\"✅ Custom neck registered with TerraTorch!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:26:41.423056Z",
     "iopub.status.busy": "2025-09-17T14:26:41.422773Z",
     "iopub.status.idle": "2025-09-17T14:26:53.923562Z",
     "shell.execute_reply": "2025-09-17T14:26:53.922773Z",
     "shell.execute_reply.started": "2025-09-17T14:26:41.423036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prithvi model enhanced with crop-specific VI attention!\n",
      "✅ All your original settings preserved\n",
      "✅ Attention applied to VI bands based on crop importance\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Correct import\n",
    "# from terratorch.registry import NECK_REGISTRY\n",
    "\n",
    "# # Register the custom neck\n",
    "# @NECK_REGISTRY.register()\n",
    "# class CropVIAttentionNeck(nn.Module):\n",
    "#     \"\"\"Crop-specific VI attention neck for Prithvi model\"\"\"\n",
    "    \n",
    "#     def __init__(self, in_channels=17, num_classes=6):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Your exact VI importance weights\n",
    "#         self.crop_vi_importance = {\n",
    "#             0: [0.9, 0.8, 0.8, 0.8, 0.8, 0.7, 0.7, 0.0, 0.6, 0.8, 0.7],  # Gram\n",
    "#             1: [0.9, 1.0, 0.7, 0.7, 0.7, 0.8, 0.8, 0.0, 0.6, 0.8, 0.7],  # Maize  \n",
    "#             2: [0.8, 0.8, 0.6, 0.7, 0.7, 0.6, 0.6, 1.0, 0.7, 0.9, 0.7],  # Mustard\n",
    "#             3: [1.0, 0.9, 0.7, 0.8, 0.8, 0.7, 0.7, 0.0, 0.6, 0.8, 0.7],  # Sugarcane\n",
    "#             4: [0.9, 0.8, 0.7, 0.7, 0.7, 0.8, 0.7, 0.0, 0.6, 1.0, 0.8],  # Wheat\n",
    "#             5: [0.8, 0.7, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.7, 0.6]   # Other\n",
    "#         }\n",
    "        \n",
    "#         # Convert to tensor [6, 11] for VI bands (indices 6-16)\n",
    "#         vi_weights = torch.tensor([self.crop_vi_importance[i] for i in range(6)], dtype=torch.float32)\n",
    "#         self.register_buffer('vi_importance_weights', vi_weights)\n",
    "        \n",
    "#         # Simple attention mechanism\n",
    "#         self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.channel_attention = nn.Sequential(\n",
    "#             nn.Linear(in_channels, max(in_channels // 4, 1)),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(max(in_channels // 4, 1), in_channels),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         B, C, H, W = x.shape\n",
    "        \n",
    "#         # Global average pooling for channel attention\n",
    "#         global_features = self.global_pool(x).squeeze(-1).squeeze(-1)  # [B, 17]\n",
    "        \n",
    "#         # Base channel attention\n",
    "#         channel_weights = self.channel_attention(global_features)  # [B, 17]\n",
    "        \n",
    "#         # Apply VI importance to vegetation index bands (6-16)\n",
    "#         avg_vi_importance = torch.mean(self.vi_importance_weights, dim=0)  # [11]\n",
    "#         channel_weights[:, 6:] = channel_weights[:, 6:] * avg_vi_importance\n",
    "        \n",
    "#         # Reshape and apply attention\n",
    "#         attention = channel_weights.unsqueeze(-1).unsqueeze(-1)  # [B, 17, 1, 1]\n",
    "#         attended_features = x * attention\n",
    "        \n",
    "#         return attended_features\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MODIFY YOUR MODEL ARGS - Replace your neck definition\n",
    "# =============================================================================\n",
    "data_module = datamodule\n",
    "# Keep everything exactly the same, just modify the necks part\n",
    "backbone_args = dict(\n",
    "    backbone_pretrained=True,\n",
    "    backbone=\"prithvi_eo_v2_600_tl\",  # Your exact backbone\n",
    "    backbone_bands=BANDS,  # Your exact bands\n",
    "    backbone_num_frames=1,\n",
    ")\n",
    "\n",
    "decoder_args = dict(\n",
    "    decoder=\"UperNetDecoder\",  # Your exact decoder\n",
    "    decoder_channels=256,\n",
    "    decoder_scale_modules=True\n",
    ")\n",
    "\n",
    "# MODIFIED: Add crop attention neck before the reshape neck\n",
    "necks = [\n",
    "    # First apply crop attentionxq\n",
    "    dict(\n",
    "        name=\"create_crop_vi_attention_neck\",  # Use the registered class name\n",
    "        \n",
    "    ),\n",
    "    # Then reshape as before\n",
    "    dict(\n",
    "        name=\"ReshapeTokensToImage\",\n",
    "        effective_time_dim=NUM_FRAMES,\n",
    "    )\n",
    "]\n",
    "# Keep your exact model args, just with new necks\n",
    "model_args = dict(\n",
    "    **backbone_args,\n",
    "    **decoder_args,\n",
    "    num_classes=6,  # Your exact classes\n",
    "    head_dropout=HEAD_DROPOUT,  # Your exact dropout\n",
    "    necks=necks,  # Modified necks with attention\n",
    "    rescale=True\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# KEEP YOUR EXACT MODEL CREATION - No changes needed!\n",
    "# =============================================================================\n",
    "\n",
    "model = CustomSegmentationTaskWithEQLv2(\n",
    "    model_args=model_args,\n",
    "    plot_on_val=False,\n",
    "    class_weights=CLASS_WEIGHTS,  # Your exact class weights\n",
    "    # loss=\"dice\",  # Your exact loss\n",
    "    lr=LR,  # Your exact learning rate\n",
    "    optimizer=\"AdamW\",  # Your exact optimizer\n",
    "    optimizer_hparams=dict(weight_decay=WEIGHT_DECAY),  # Your exact weight decay\n",
    "    freeze_backbone=FREEZE_BACKBONE,  # Your exact freeze setting\n",
    "    freeze_decoder=False,\n",
    "    model_factory=\"EncoderDecoderFactory\",  # Your exact factory\n",
    "    ignore_index=-1,\n",
    "    scheduler='CosineAnnealingLR',\n",
    "    scheduler_hparams={\n",
    "        \"T_max\": 120,\n",
    "        \"eta_min\": 1e-8\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✅ Prithvi model enhanced with crop-specific VI attention!\")\n",
    "print(\"✅ All your original settings preserved\")\n",
    "print(\"✅ Attention applied to VI bands based on crop importance\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING - Keep exactly the same!\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-17T14:22:47.175218Z",
     "iopub.status.idle": "2025-09-17T14:22:47.175516Z",
     "shell.execute_reply": "2025-09-17T14:22:47.175355Z",
     "shell.execute_reply.started": "2025-09-17T14:22:47.175340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:26:53.925102Z",
     "iopub.status.busy": "2025-09-17T14:26:53.924842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model         | PixelWiseModel   | 683 M  | train\n",
      "1 | criterion     | FocalTverskyLoss | 0      | train\n",
      "2 | train_metrics | MetricCollection | 0      | train\n",
      "3 | val_metrics   | MetricCollection | 0      | train\n",
      "4 | test_metrics  | ModuleList       | 0      | train\n",
      "-----------------------------------------------------------\n",
      "683 M     Trainable params\n",
      "0         Non-trainable params\n",
      "683 M     Total params\n",
      "2,732.734 Total estimated model params size (MB)\n",
      "951       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c4f8128ce34a4d8044a87da5ca0d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 0, global step 31: 'val/Multiclass_Jaccard_Index' reached 0.18225 (best 0.18225), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=00-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 0, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 1, global step 62: 'val/Multiclass_Jaccard_Index' reached 0.21903 (best 0.21903), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=01-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 1, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 2, global step 93: 'val/Multiclass_Jaccard_Index' reached 0.23261 (best 0.23261), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=02-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 2, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 3, global step 124: 'val/Multiclass_Jaccard_Index' reached 0.27145 (best 0.27145), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=03-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 3, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 4, global step 155: 'val/Multiclass_Jaccard_Index' reached 0.28611 (best 0.28611), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=04-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 4, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 5, global step 186: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 5, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 6, global step 217: 'val/Multiclass_Jaccard_Index' reached 0.28795 (best 0.28795), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=06-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 6, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 7, global step 248: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 7, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 8, global step 279: 'val/Multiclass_Jaccard_Index' reached 0.29992 (best 0.29992), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=08-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 8, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 9, global step 310: 'val/Multiclass_Jaccard_Index' reached 0.30932 (best 0.30932), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=09-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 9, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 10, global step 341: 'val/Multiclass_Jaccard_Index' reached 0.31409 (best 0.31409), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=10-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 10, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 11, global step 372: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 11, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 12, global step 403: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 12, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 13, global step 434: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 13, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 14, global step 465: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 14, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 15, global step 496: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 15, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cb08ea71d947c7b77e6383a7cff3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 16, global step 527: 'val/Multiclass_Jaccard_Index' reached 0.31499 (best 0.31499), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=16-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 16, Current LR: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49003ad0f1244fe78412e7435d4467f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 17, global step 558: 'val/Multiclass_Jaccard_Index' reached 0.31823 (best 0.31823), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=17-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 17, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 18, global step 589: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 18, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 19, global step 620: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 19, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 20, global step 651: 'val/Multiclass_Jaccard_Index' reached 0.31966 (best 0.31966), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=20-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 20, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 21, global step 682: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 21, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 22, global step 713: 'val/Multiclass_Jaccard_Index' reached 0.34069 (best 0.34069), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=22-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 22, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 23, global step 744: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 23, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 24, global step 775: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 24, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 25, global step 806: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 25, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 26, global step 837: 'val/Multiclass_Jaccard_Index' reached 0.37099 (best 0.37099), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=26-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 26, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 27, global step 868: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 27, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 28, global step 899: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 28, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 29, global step 930: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 29, Current LR: 0.000009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 30, global step 961: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 30, Current LR: 0.000008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 31, global step 992: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 31, Current LR: 0.000008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 32, global step 1023: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 32, Current LR: 0.000008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 33, global step 1054: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 33, Current LR: 0.000008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 34, global step 1085: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 34, Current LR: 0.000008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 35, global step 1116: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 35, Current LR: 0.000008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 36, global step 1147: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 36, Current LR: 0.000008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 37, global step 1178: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 37, Current LR: 0.000008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 38, global step 1209: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 38, Current LR: 0.000008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 39, global step 1240: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 39, Current LR: 0.000008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 40, global step 1271: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 40, Current LR: 0.000007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 41, global step 1302: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 41, Current LR: 0.000007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 42, global step 1333: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 42, Current LR: 0.000007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 43, global step 1364: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 43, Current LR: 0.000007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 44, global step 1395: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 44, Current LR: 0.000007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 45, global step 1426: 'val/Multiclass_Jaccard_Index' reached 0.37188 (best 0.37188), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=45-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 45, Current LR: 0.000007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 46, global step 1457: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 46, Current LR: 0.000007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 47, global step 1488: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 47, Current LR: 0.000007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 48, global step 1519: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 48, Current LR: 0.000006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 49, global step 1550: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 49, Current LR: 0.000006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 50, global step 1581: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 50, Current LR: 0.000006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 51, global step 1612: 'val/Multiclass_Jaccard_Index' reached 0.38723 (best 0.38723), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=51-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 51, Current LR: 0.000006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 52, global step 1643: 'val/Multiclass_Jaccard_Index' reached 0.41996 (best 0.41996), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=52-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 52, Current LR: 0.000006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 53, global step 1674: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 53, Current LR: 0.000006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 54, global step 1705: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 54, Current LR: 0.000006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 55, global step 1736: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 55, Current LR: 0.000006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 56, global step 1767: 'val/Multiclass_Jaccard_Index' reached 0.44907 (best 0.44907), saving model to '/kaggle/working/cropid/checkpoints/best-checkpoint-epoch=56-val_loss=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 56, Current LR: 0.000005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 57, global step 1798: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 57, Current LR: 0.000005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 58, global step 1829: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 58, Current LR: 0.000005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 59, global step 1860: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 59, Current LR: 0.000005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 60, global step 1891: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 60, Current LR: 0.000005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 61, global step 1922: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 61, Current LR: 0.000005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 62, global step 1953: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 62, Current LR: 0.000005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 63, global step 1984: 'val/Multiclass_Jaccard_Index' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 63, Current LR: 0.000004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    precision=\"16-mixed\",\n",
    "    num_nodes=1,\n",
    "    logger=logger,\n",
    "    max_epochs=EPOCHS,\n",
    "    check_val_every_n_epoch=1,\n",
    "    log_every_n_steps=5,\n",
    "    accumulate_grad_batches=3,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.test(dataloaders=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zip_folder(folder_path, output_path):\n",
    "    \"\"\"\n",
    "    Zips the entire contents of a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder you want to zip.\n",
    "        output_path (str): The path for the output zip file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            # os.walk() generates the file names in a directory tree\n",
    "            for root, dirs, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    # Create the full path of the file\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    # Create a relative path for the file inside the zip archive\n",
    "                    # This keeps the folder structure intact without the top-level path\n",
    "                    archive_path = os.path.relpath(file_path, folder_path)\n",
    "                    zipf.write(file_path, arcname=archive_path)\n",
    "        \n",
    "        print(f\"Successfully created zip file: {output_path} ✅\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Define the folder you want to zip\n",
    "    folder_to_zip = '/kaggle/working/crop'\n",
    "    \n",
    "    # 2. Define the name and path for the output zip file\n",
    "    output_zip_filename = '/kaggle/working/crop_log.zip'\n",
    "        \n",
    "    # 3. Call the function to zip the folder\n",
    "    zip_folder(folder_to_zip, output_zip_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_dir = \"/kaggle/working/cropid/checkpoints\"\n",
    "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"best-checkpoint-*.ckpt\"))\n",
    "\n",
    "if len(checkpoint_files) == 0:\n",
    "    raise FileNotFoundError(\"No best checkpoint file found in the directory.\")\n",
    "    \n",
    "# Use the first match\n",
    "best_ckpt_path = checkpoint_files[0]\n",
    "# test_results = trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_path)\n",
    "# print(test_results)\n",
    "print(best_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# A. Saving predictions on test input images  \n",
    " \n",
    "# Prediction on predict_data_root (see dataloader part)\n",
    "preds = trainer.predict(model, datamodule=datamodule, ckpt_path=best_ckpt_path)\n",
    "\n",
    "output_dataset_path = \"/kaggle/working/\"\n",
    "# Output directory to save prediction tif files\n",
    "output_dir = os.path.join(output_dataset_path, \"test_pred\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    " \n",
    "# Loop over batches\n",
    "for batch_idx, batch in enumerate(preds):\n",
    "    tensor = batch[0][0]    # shape [4, 256, 256] → 4 images in this batch\n",
    "    file_paths = batch[1]   # list of file paths for this batch\n",
    " \n",
    "    for i in range(tensor.shape[0]):  # loop over each file in batch\n",
    "        arr = tensor[i].cpu().numpy().astype('int32')  # shape [256, 256] → single band\n",
    " \n",
    "        ref_path = file_paths[i]\n",
    "        with rasterio.open(ref_path) as src_ref:\n",
    "            ref_crs = src_ref.crs\n",
    "            ref_transform = src_ref.transform\n",
    " \n",
    "        out_name = os.path.splitext(os.path.basename(ref_path))[0] + \"_pred.tif\"\n",
    "        out_path = os.path.join(output_dir, out_name)\n",
    " \n",
    "        with rasterio.open(\n",
    "            out_path,\n",
    "            \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=arr.shape[0],\n",
    "            width=arr.shape[1],\n",
    "            count=1,\n",
    "            dtype=arr.dtype,\n",
    "            crs=ref_crs,\n",
    "            transform=ref_transform\n",
    "        ) as dst:\n",
    "            dst.write(arr, 1)\n",
    " \n",
    "        print(f\"Saved {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Input directories\n",
    "# dir_pred = os.path.join(dataset_path, \"test_pred\")\n",
    "# dir_mask = os.path.join(dataset_path, \"test\")\n",
    "# prediction_csv = os.path.join(dataset_path, \"submission.csv\")\n",
    " \n",
    "# dir_pred = os.path.join(dataset_path, \"test\")\n",
    "# dir_mask = os.path.join(OUT_DIR, \"\")\n",
    "# prediction_csv = os.path.join(OUT_DIR, \"test_labels_2.csv\")\n",
    " \n",
    "# Input directories\n",
    "dir_pred = os.path.join(output_dataset_path, \"test_pred\")\n",
    "dir_mask = os.path.join(dataset_path, \"test/labels\")\n",
    "prediction_csv = os.path.join(output_dataset_path, \"prediction_latest1.csv\")\n",
    " \n",
    "def mask_to_rle(mask):   \n",
    "    mask = mask.flatten(order='F')\n",
    "    pixels = np.concatenate([[0], mask, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(str(x) for x in runs)\n",
    " \n",
    " \n",
    "records = []\n",
    " \n",
    "# Get prediction files\n",
    "pred_files = glob.glob(os.path.join(dir_pred, \"*.tif\"))\n",
    "# pred_files = glob.glob(os.path.join(dir_pred, \"*label_c6.tif\"))\n",
    " \n",
    "for pred_file in pred_files:\n",
    "    base_name = os.path.basename(pred_file)\n",
    "    mask_file = os.path.join(dir_mask, base_name.replace(\"input_pred.tif\", \"label_mask.tif\"))\n",
    "    # mask_file = os.path.join(dir_mask, base_name.replace(\"label_c6.tif\", \"label_mask.tif\"))\n",
    "    \n",
    "    if not os.path.exists(mask_file):\n",
    "        print(f\"No mask found for {base_name}, skipping\")\n",
    "        continue\n",
    " \n",
    "    # Read prediction\n",
    "    with rasterio.open(pred_file) as src_pred:\n",
    "        pred_data = src_pred.read(1)\n",
    " \n",
    "    # Read mask\n",
    "    with rasterio.open(mask_file) as src_mask:\n",
    "        mask_data = src_mask.read(1)\n",
    " \n",
    "    # Apply mask (only keep pixels where mask == 1)\n",
    "    valid_idx = np.where(mask_data == 1)\n",
    " \n",
    "    # Pixel IDs\n",
    "    pixel_ids = np.ravel_multi_index(valid_idx, pred_data.shape)\n",
    " \n",
    "    # Predictions on masked pixels\n",
    "    masked_preds = pred_data[valid_idx]\n",
    "    # masked_preds = pred_data\n",
    " \n",
    "    # For each class (0–5)\n",
    "    flat_size = pred_data.size\n",
    "    for cls in range(6):\n",
    "        cls_mask = np.zeros(flat_size, dtype=np.uint8)\n",
    "        cls_mask[pixel_ids[masked_preds == cls]] = 1\n",
    "        # Select pixels belonging to this class\n",
    "        # cls_pixels = pixel_ids[masked_preds == cls]\n",
    " \n",
    "        # if cls_pixels.size > 0:\n",
    "        #     # Format as \"pixel_id class\" pairs\n",
    "        #     pred_str = \" \".join(f\"{pid} {cls}\" for pid in cls_pixels.tolist())\n",
    "        # else:\n",
    "        #     pred_str = \"\"\n",
    "        if cls_mask.sum() > 0:\n",
    "            pred_str = mask_to_rle(cls_mask)\n",
    "        else:\n",
    "            pred_str = \"0\"\n",
    " \n",
    "        # Remove \"_input_pred.tif\" and add class as suffix\n",
    "        file_id = base_name.replace(\"_input_pred.tif\", f\"_{cls}\")\n",
    "        # file_id = base_name.replace(\"_label_c6.tif\", f\"_{cls}\")\n",
    " \n",
    "        records.append([file_id, pred_str])\n",
    " \n",
    "# Create df\n",
    "df = pd.DataFrame(records, columns=[\"id\", \"label\"])\n",
    "\n",
    "#df[\"label\"] = df[\"label\"].fillna(\"0\")\n",
    " \n",
    "# Save CSV\n",
    "df.to_csv(prediction_csv, index=False)\n",
    " \n",
    "print(f\"Saved predictions to {prediction_csv}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8219064,
     "sourceId": 12985400,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 260770766,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
